{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b522f5",
   "metadata": {},
   "source": [
    "## Building Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f17d5",
   "metadata": {},
   "source": [
    "The steps to building and using a model are:\n",
    "\n",
    "- Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n",
    "- Fit: Capture patterns from provided data. This is the heart of modeling.\n",
    "- Predict: Just what it sounds like\n",
    "- Evaluate: Determine how accurate the model's predictions are.\n",
    "\n",
    "Here is an example of defining a decision tree model with scikit-learn and fitting it with the features and target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea67e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "file_path = 'CSV_path' # file location\n",
    "\n",
    "home_data = pd.read_csv(\"file_path\") # read csv input file\n",
    "home_data.describe() # print all csv data as a dataframe\n",
    "home_data.head() # see first 5 values in home_data\n",
    "home_data.columns # see list of all column names\n",
    "\n",
    "y = home_data['Target'] # as per convention, y='' is used to pick the target column to be predicted by the model\n",
    "# Create the list of features below\n",
    "feature_names = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd'] #example names\n",
    "\n",
    "# Select data corresponding to features in feature_names\n",
    "X = home_data[feature_names]\n",
    "X.head() #review data before fitting the model\n",
    "\n",
    "#define the model type\n",
    "model = DecisionTreeRegressor(random_state=1)\n",
    "# Fit model\n",
    "model.fit(X, y)\n",
    "\n",
    "#making and printing the model predictions\n",
    "predictions = model.predict(X) \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298164a",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d283f2",
   "metadata": {},
   "source": [
    "There are many metrics for summarizing model quality, but we'll start with one called Mean Absolute Error (also called MAE).\n",
    "\n",
    "error = actualâˆ’predicted. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aabe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predicted_home_prices = melbourne_model.predict(X)\n",
    "mean_absolute_error(y, predicted_home_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d8811",
   "metadata": {},
   "source": [
    "### The Problem with \"In-Sample\" Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f4963a",
   "metadata": {},
   "source": [
    "The measure we just computed can be called an \"in-sample\" score. We used a single \"sample\" of houses for both building the model and evaluating it. Here's why this is bad.\n",
    "\n",
    "Imagine that, in the large real estate market, door color is unrelated to home price.\n",
    "\n",
    "However, in the sample of data you used to build the model, all homes with green doors were very expensive. The model's job is to find patterns that predict home prices, so it will see this pattern, and it will always predict high prices for homes with green doors.\n",
    "\n",
    "Since this pattern was derived from the training data, the model will appear accurate in the training data.\n",
    "\n",
    "But if this pattern doesn't hold when the model sees new data, the model would be very inaccurate when used in practice.\n",
    "\n",
    "Since models' practical value come from making predictions on new data, we measure performance on data that wasn't used to build the model. The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model's accuracy on data it hasn't seen before. This data is called validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b73b6",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e303cff",
   "metadata": {},
   "source": [
    "The scikit-learn library has a function train_test_split to break up the data into two pieces. We'll use some of that data as training data to fit the model, and we'll use the other data as validation data to calculate mean_absolute_error.\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aebc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "# The split is based on a random number generator. Supplying a numeric value to\n",
    "# the random_state argument guarantees we get the same split every time we\n",
    "# run this script.\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "# Define model\n",
    "melbourne_model = DecisionTreeRegressor()\n",
    "# Fit model\n",
    "melbourne_model.fit(train_X, train_y)\n",
    "\n",
    "# get predicted prices on validation data\n",
    "val_predictions = melbourne_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0cd93f",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c7a4a",
   "metadata": {},
   "source": [
    "Models can suffer from either:\n",
    "\n",
    "- Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or\n",
    "- Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.\n",
    "\n",
    "We use validation data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef03e86",
   "metadata": {},
   "source": [
    "For finding the sweet spot between underfitting and overfitting, we can try many different things. In the above example of a decision tree model, one method is controlling tree depth. There are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n",
    "\n",
    "We can use a utility function to help compare MAE scores from different values for max_leaf_nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a67a40",
   "metadata": {},
   "source": [
    "We can use a for-loop to compare the accuracy of models built with different values for max_leaf_nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88adcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare MAE with differing values of max_leaf_nodes\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e3b00",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ca721",
   "metadata": {},
   "source": [
    "Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n",
    "\n",
    "Even today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the random forest as an example.\n",
    "\n",
    "The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters.\n",
    "\n",
    "We build a random forest model similarly to how we built a decision tree in scikit-learn - this time using the RandomForestRegressor class instead of DecisionTreeRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c138b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, melb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0f52b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
