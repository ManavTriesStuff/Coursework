{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18737fb",
   "metadata": {},
   "source": [
    "# AI Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30cfc9e",
   "metadata": {},
   "source": [
    "## Human-centred Design for AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0ef5c",
   "metadata": {},
   "source": [
    "Before selecting data and training models, it is important to carefully consider the human needs an AI system should serve - and if it should be built at all. Human-centered design (HCD) is an approach to designing systems that serve people’s needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5650763d",
   "metadata": {},
   "source": [
    "### Understand people’s needs to define the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd2eee2",
   "metadata": {},
   "source": [
    "Working with people to understand the pain points in their current journeys can help find unaddressed needs. This can be done by observing people as they navigate existing tools, conducting interviews, assembling focus groups, reading user feedback and other methods. Your entire team – including data scientists and engineers – should be involved in this step, so that every team member gains an understanding of the people they hope to serve. Your team should include and involve people with diverse perspectives and backgrounds, along race, gender, and other characteristics. Sharpen your problem definition and brainstorm creative and inclusive solutions together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c21d85",
   "metadata": {},
   "source": [
    "Example:\n",
    "    \n",
    "A company wants to address the problem of dosage errors for immunosuppressant drugs given to patients after liver transplants. The company starts by observing physicians, nurses and other hospital staff throughout the liver transplant process. It also interviews them about the current dosage determination process - which relies on published guidelines and human judgment - and shares video clips from the interviews with the entire development team. The company also reviews research studies and assembles focus groups of former patients and their families. All team members participate in a freewheeling brainstorming session for potential solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd7d79",
   "metadata": {},
   "source": [
    "### Ask if AI adds value to any potential solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc85b78",
   "metadata": {},
   "source": [
    "Once you are clear about which need you are addressing and how, consider whether AI adds value.\n",
    "\n",
    "- Would people generally agree that what you are trying to achieve is a good outcome?\n",
    "- Would non-AI systems - such as rule-based solutions, which are easier to create, audit and maintain - be significantly less effective than an AI system?\n",
    "- Is the task that you are using AI for one that people would find boring, repetitive or otherwise difficult to concentrate on?\n",
    "- Have AI solutions proven to be better than other solutions for similar use cases in the past?\n",
    "\n",
    "If you answered no to any of these questions, an AI solution may not be necessary or appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba595d9",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "A disaster response agency is working with first responders to reduce the time it takes to rescue people from disasters, like floods. The time- and labor-intensive human review of drone and satellite photos to find stranded people increases rescue time. Everybody agrees that speeding up photo review would be a good outcome, since faster rescues could save more lives. The agency determines that an AI image recognition system would likely be more effective than a non-AI automated system for this task. It is also aware that AI-based image recognition tools have been applied successfully to review aerial footage in other industries, like agriculture. The agency therefore decides to further explore the possibility of an AI-based solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc8f7d",
   "metadata": {},
   "source": [
    "### Consider the potential harms that the AI system could cause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea86f97",
   "metadata": {},
   "source": [
    "Weigh the benefits of using AI against the potential harms, throughout the design pipeline: from collecting and labeling data, to training a model, to deploying the AI system. Consider the impact on users and on society. Your privacy team can help uncover hidden privacy issues and determine whether privacy-preserving techniques like differential privacy or federated learning may be appropriate. Take steps to reduce harms, including by embedding people - and therefore human judgment - more effectively in data selection, in model training and in the operation of the system. If you estimate that the harms are likely to outweigh the benefits, do not build the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e21959",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "An online education company wants to use an AI system to ‘read’ and automatically assign scores to student essays, while redirecting company staff to double-check random essays and to review essays that the AI system has trouble with. The system would enable the company to quickly get scores back to students. The company creates a harms review committee, which recommends that the system not be built. Some of the major harms flagged by the committee include: the potential for the AI system to pick up bias against certain patterns of language from training data and amplify it (harming people in the groups that use those patterns of language), to encourage students to ‘game’ the algorithm rather than improve their essays and to reduce the classroom role of education experts while increasing the role of technology experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950aaf2",
   "metadata": {},
   "source": [
    "### Prototype, starting with non-AI solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0075c8",
   "metadata": {},
   "source": [
    "Develop a non-AI prototype of your AI system quickly to see how people interact with it. This makes prototyping easier, faster and less expensive. It also gives you early information about what users expect from your system and how to make their interactions more rewarding and meaningful. Design your prototype’s user interface to make it easy for people to learn how your system works, to toggle settings and to provide feedback. The people giving feedback should have diverse backgrounds – including along race, gender, expertise and other characteristics. They should also understand and consent to what they are helping with and how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6f3bc",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "A movie streaming startup wants to use AI to recommend movies to users, based on their stated preferences and viewing history. The team first invites a diverse group of users to share their stated preferences and viewing history with a movie enthusiast, who then recommends movies that the users might like. Based on these conversations and on feedback about which recommended movies users enjoyed, the team changes its approach to how movies are categorized. Getting feedback from a diverse group of users early and iterating often allows the team to improve its product early, rather than making expensive corrections later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce6a18",
   "metadata": {},
   "source": [
    "### Provide ways for people to challenge the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21261c6d",
   "metadata": {},
   "source": [
    "People who use your AI system once it is live should be able to challenge its recommendations or easily opt out of using it. Put systems and tools in place to accept, monitor and address challenges. Talk to users and think from the perspective of a user: if you are curious or dissatisfied with the system’s recommendations, would you want to challenge it by:\n",
    "\n",
    "- Requesting an explanation of how it arrived at its recommendation?\n",
    "- Requesting a change in the information you input?\n",
    "- Turning off certain features?\n",
    "- Reaching out to the product team on social media?\n",
    "- Taking some other action?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d87720",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "An online video conferencing company uses AI to automatically blur the background during video calls. The company has successfully tested its product with a diverse group of people from different ethnicities. Still, it knows that there could be instances in which the video may not properly focus on a person’s face. So, it makes the background blurring feature optional and adds a button for customers to report issues. The company also creates a customer service team to monitor social media and other online forums for user complaints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f628911",
   "metadata": {},
   "source": [
    "###  Build in safety measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be633913",
   "metadata": {},
   "source": [
    "Safety measures protect users against harm. They seek to limit unintended behavior and accidents, by ensuring that a system reliably delivers high-quality outcomes. This can only be achieved through extensive and continuous evaluation and testing. Design processes around your AI system to continuously monitor performance, delivery of intended benefits, reduction of harms, fairness metrics and any changes in how people are actually using it. The kind of safety measures your system needs depends on its purpose and on the types of harms it could cause. Start by reviewing the list of safety measures built into similar non-AI products or services. Then, review your earlier analysis of the potential harms of using AI in your system (see Step 3).\n",
    "\n",
    "Human oversight of your AI system is crucial:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca8045",
   "metadata": {},
   "source": [
    "- Create a human ‘red team’ to play the role of a person trying to manipulate your system into unintended behavior. Then, strengthen your system against any such manipulation.\n",
    "- Determine how people in your organization can best monitor the system’s safety once it is live.\n",
    "- Explore ways for your AI system to quickly alert a human when it is faced with a challenging case.\n",
    "- Create ways for users and others to flag potential safety issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07b8ff",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "To bolster the safety of its product, a company that develops a widely-used AI-enabled voice assistant creates a permanent internal ‘red team’ to play the role of bad actors that want to manipulate the voice assistant. The red team develops adversarial inputs to fool the voice assistant. The company then uses ‘adversarial training’ to guard the product against similar adversarial inputs, improving its safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db85729",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174775ad",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90460695",
   "metadata": {},
   "source": [
    "Bias refers to negative, unwanted consequences of ML applications, especially if the consequences disproportionately affect certain groups. Machine learning (ML) has the potential to improve lives, but it can also be a source of harm. ML applications have discriminated against individuals on the basis of race, sex, religion, socioeconomic status, and other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249066f",
   "metadata": {},
   "source": [
    "#### 6 Types of Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec677cfc",
   "metadata": {},
   "source": [
    "- Historical bias (occurs when the state of the world in which the data was generated is flawed)\n",
    "Example: As of 2020, only 7.4% of Fortune 500 CEOs are women. Research has shown that companies with female CEOs or CFOs are generally more profitable than companies with men in the same position, suggesting that women are held to higher hiring standards than men. In order to fix this, we might consider removing human input and using AI to make the hiring process more equitable. But this can prove unproductive if data from past hiring decisions is used to train a model, because the model will likely learn to demonstrate the same biases that are present in the data.\n",
    "- Representation bias (occurs when building datasets for training a model, if those datasets poorly represent the people that the model will serve)\n",
    "Example: Data collected through smartphone apps will under-represent groups that are less likely to own smartphones. For instance, if collecting data in the USA, individuals over the age of 65 will be under-represented. If the data is used to inform design of a city transportation system, this will be disastrous, since older people have important needs to ensure that the system is accessible.\n",
    "- Measurement bias (occurs when the accuracy of the data varies across groups. This can happen when working with proxy variables ie: variables that take the place of a variable that cannot be directly measured, if the quality of the proxy varies in different groups)\n",
    "Example: Your local hospital uses a model to identify high-risk patients before they develop serious conditions, based on information like past diagnoses, medications, and demographic data. The model uses this information to predict health care costs, the idea being that patients with higher costs likely correspond to high-risk patients. Despite the fact that the model specifically excludes race, it seems to demonstrate racial discrimination: the algorithm is less likely to select eligible Black patients. How can this be the case? It is because cost was used as a proxy for risk, and the relationship between these variables varies with race: Black patients experience increased barriers to care, have less trust in the health care system, and therefore have lower medical costs, on average, when compared to non-Black patients with the same health conditions.\n",
    "- Aggregation bias occurs when groups are inappropriately combined, resulting in a model that does not perform well for any group or only performs well for the majority group. (This is often not an issue, but most commonly arises in medical applications.)\n",
    "Example: Hispanics have higher rates of diabetes and diabetes-related complications than non-Hispanic whites. If building AI to diagnose or monitor diabetes, it is important to make the system sensitive to these ethnic differences, by either including ethnicity as a feature in the data, or building separate models for different ethnic groups.\n",
    "- Evaluation bias occurs when evaluating a model, if the benchmark data (used to compare the model to other models that perform similar tasks) does not represent the population that the model will serve.\n",
    "Example: The Gender Shades paper discovered that two widely used facial analysis benchmark datasets (IJB-A and Adience) were primarily composed of lighter-skinned subjects (79.6% and 86.2%, respectively). Commercial gender classification AI showed state-of-the-art performance on these benchmarks, but experienced disproportionately high error rates with people of color.\n",
    "- Deployment bias occurs when the problem the model is intended to solve is different from the way it is actually used. If the end users don’t use the model in the way it is intended, there is no guarantee that the model will perform well.\n",
    "Example: The criminal justice system uses tools to predict the likelihood that a convicted criminal will relapse into criminal behavior. The predictions are not designed for judges when deciding appropriate punishments at the time of sentencing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2139a",
   "metadata": {},
   "source": [
    "Note: these are not mutually exclusive: that is, an ML application can easily suffer from more than one type of bias. For example, as Rachel Thomas describes in a recent research talk, ML applications in wearable fitness devices can suffer from:\n",
    "\n",
    "- Representation bias (if the dataset used to train the models exclude darker skin tones),\n",
    "- Measurement bias (if the measurement apparatus shows reduced performance with dark skin tones), and\n",
    "- Evaluation bias (if the dataset used to benchmark the model excludes darker skin tones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4debbf",
   "metadata": {},
   "source": [
    "### AI Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca443e6",
   "metadata": {},
   "source": [
    "Assume we're working with a model that selects individuals to receive some outcome. For instance, the model could select people who should be approved for a loan, accepted to a university, or offered a job opportunity. (So, we don't consider models that perform tasks like facial recognition or text generation, among other things.) These are 4 major fairness criteria that the model could be evaluated on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645cd3e",
   "metadata": {},
   "source": [
    "- Demographic/Statistical parity says the model is fair if the composition of people who are selected by the model matches the group membership percentages of the applicants.\n",
    "Example: Demographic parity says the model is fair if the composition of people who are selected by the model matches the group membership percentages of the applicants.\n",
    "- Equal opportunity fairness ensures that the proportion of people who should be selected by the model (\"positives\") that are correctly selected by the model is the same for each group. We refer to this proportion as the true positive rate (TPR) or sensitivity of the model.\n",
    "Example: A doctor uses a tool to identify patients in need of extra care, who could be at risk for developing serious medical conditions. (This tool is used only to supplement the doctor's practice, as a second opinion.) It is designed to have a high TPR that is equal for each demographic group.\n",
    "- Alternatively, we could check that the model has equal accuracy for each group. That is, the percentage of correct classifications (people who should be denied and are denied, and people who should be approved who are approved) should be the same for each group. If the model is 98% accurate for individuals in one group, it should be 98% accurate for other groups.\n",
    "Example: A bank uses a model to approve people for a loan. The model is designed to be equally accurate for each demographic group: this way, the bank avoids approving people who should be rejected (which would be financially damaging for both the applicant and the bank) and avoid rejecting people who should be approved (which would be a failed opportunity for the applicant and reduce the bank's revenue).\n",
    "- Group unaware fairness removes all group membership information from the dataset. For instance, we can remove gender data to try to make the model fair to different gender groups. Similarly, we can remove information about race or age.\n",
    "Example: One difficulty of applying this approach in practice is that one has to be careful to identify and remove proxies for the group membership data. For instance, in cities that are racially segregated, zip code is a strong proxy for race. That is, when the race data is removed, the zip code data should also be removed, or else the ML application may still be able to infer an individual's race from the data. Additionally, group unaware fairness is unlikely to be a good solution for historical bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae0235",
   "metadata": {},
   "source": [
    "### Model Cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b7be2f",
   "metadata": {},
   "source": [
    "A model card is a short document that provides key information about a machine learning model. Model cards increase transparency by communicating information about trained models to broad audiences. Per the original paper, a model card should have the following nine sections. Note that different organizations may add, subtract or rearrange model card sections according to their needs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27270861",
   "metadata": {},
   "source": [
    "1. Model Details\n",
    "Include background information, such as developer and model version.\n",
    "2. Intended Use\n",
    "What use cases are in scope?\n",
    "Who are your intended users?\n",
    "What use cases are out of scope?\n",
    "3. Factors\n",
    "What factors affect the impact of the model? For example, the smiling detection model's results vary by demographic factors like age, gender or ethnicity, environmental factors like lighting or rain and instrumentation like camera type.\n",
    "4. Metrics\n",
    "What metrics are you using to measure the performance of the model? Why did you pick those metrics?\n",
    "For classification systems – in which the output is a class label – potential error types include false positive rate, false negative rate, false discovery rate, and false omission rate. The relative importance of each of these depends on the use case.\n",
    "For score-based analyses – in which the output is a score or price – consider reporting model performance across groups.\n",
    "5. Evaluation Data\n",
    "Which datasets did you use to evaluate model performance? Provide the datasets if you can.\n",
    "Why did you choose these datasets for evaluation?\n",
    "Are the datasets representative of typical use cases, anticipated test cases and/or challenging cases?\n",
    "6. Training Data\n",
    "Which data was the model trained on?\n",
    "7. Quantitative Analyses\n",
    "How did the model perform on the metrics you chose? Break down performance by important factors and their intersections. For example, in the smiling detection example, performance is broken down by age (eg, young, old), gender (eg, female, male), and then both (eg, old-female, old-male, young-female, young-male).\n",
    "8. Ethical Considerations\n",
    "Describe ethical considerations related to the model, such as sensitive data used to train the model, whether the model has implications for human life, health, or safety, how risk was mitigated, and what harms may be present in model usage.\n",
    "9. Caveats and Recommendations\n",
    "Add anything important that you have not covered elsewhere in the model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16667fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
